---
# vim: set spell linebreak textwidth=0:
title: Locking out AI bots with the Dark Visitors API
description: 
author: Claudine Chionh
date: 2024-04-21T16:31:58+10:00
publishDate:
draft: all
tags: [robots.txt, darkvisitors, webdev, hugo]
---

The [Robots Exclusion Protocol](https://en.wikipedia.org/wiki/Robots.txt) was initially developed in the early years of the web to prevent small websites from being overwhelmed by search engine crawlers, and its use has expanded to exclude robots for a variety of reasons, most recently to keep artificial intelligence bots from feeding their large language models with web content. There are various hand-picked lists of known AI bots that have been circulating among web developers, and there's also [Dark Visitors](https://darkvisitors.com/), which maintains a [comprehensive list](https://darkvisitors.com/agents) of known bots, categorised by type. You can use this resource to generate your own `robots.txt`, either by copying the [example file](https://darkvisitors.com/docs/set-up-a-robots-txt), or by using the free [Dark Visitors API](https://darkvisitors.com/docs/robots-txts-api) to customise the types you want to exclude.

Hugo can generate a [`robots.txt`](https://gohugo.io/templates/robots/) file either from a Hugo template or as a static file. I'm going to use the Dark Visitors API, so I need to add `enableRobotsTXT: true` to my Hugo configuration. I'll also add `robots.txt` to `.gitignore` to keep it out of version control.

NearlyFreeSpeech provides a web-based interface to [cron](https://en.wikipedia.org/wiki/Cron) so I can schedule a periodic job to regenerate `robots.txt` from Dark Visitors – weekly seems a reasonable frequency. I've decided to only exclude bots that Dark Visitors categorises as "AI Agent", "AI Data Scraper", or "AI Search Crawler". While I was looking around my log files I discovered an [`MJ12bot`](https://darkvisitors.com/agents/mj12bot) which was making a lot of requests for weird URLs – this is known to be a [badly-behaved bot](https://boston.conman.org/2019/07/09.1).  Dark Visitors categorises `MJ12bot` as an "SEO Crawler", but I haven't decided to block all "SEO Crawlers" – **yet** – so I'll need to give this bot its own special entry in `robots.txt`. So my cron job runs a small shell script which copies the `MJ12bot` entry to `robots.txt`, then calls the Dark Visitors API to fetch all the AI bots.[^seocrawlers]

```sh
# Generate robots.txt with the Dark Visitors API.
# include_robots.txt is a separate entry for a badly-behaved bot.
# darkvisitors.hdr holds my API token.
# darkvisitors.json holds the rest of my configuration.
cp /home/conf/include_robots.txt /home/protected/www.claudinec.net/static/robots.txt
curl --verbose -H @/home/conf/darkvisitors.hdr --json @/home/conf/darkvisitors.json https://api.darkvisitors.com/robots-txts >> /home/protected/www.claudinec.net/static/robots.txt
```

But there are other **very** badly behaved bots that don't even respect `robots.txt`! Blocking them requires Apache access control rules – which might be the subject of another blog post.

[^seocrawlers]: Just typing this out, I realise that opening my doors to all other "SEO Crawlers" probably isn't worth the bother, so I might just reduce my shell script from two lines to one, and add "SEO Crawler" to the excluded bots.

